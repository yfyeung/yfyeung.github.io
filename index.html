<!DOCTYPE HTML>
<!--
	Yifan Yang (杨亦凡)
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Yifan Yang (杨亦凡)</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><h2>Yifan Yang</h2></a>
								</header>

							<!-- Banner -->
								<section id="banner">
									<span class="image object">
                                       	<img src="images/pic001.jpg" width="100px" padding-bottom="71.4%" alt=""/>
                                    </span>
									<p>
										Ph.D. student,<br />
										Shanghai Jiao Tong University.<br />
										800 Dongchuan RD. Minhang District,<br />
										Shanghai, China.
									</p>
								</section>

							<!-- Section -->
							<section>
								<header class="major">
								<h2>Biography</h2>
								</header>
								<p>
								    I am a Ph.D. student at Shanghai Jiao Tong University (SJTU), a member of <a href="https://x-lance.github.io/">Cross Media (X-)Language Intelligence Lab (X-LANCE)</a> in the Department of Computer Science and Engineering, under the supervision of Prof. <a href="https://chenxie95.github.io">Xie Chen</a>, and the leadership of Prof. <a href="https://x-lance.github.io/kaiyu">Kai Yu</a>.
								</p>
								<p>
									Previously, I worked as a Machine Learning Engineer Intern at Xiaomi AI Lab, developing <a href="https://k2-fsa.org">Next-generation Kaldi</a> under the leadership of Dr. <a href="https://scholar.google.com/citations?user=y_-5FWAAAAAJ">Daniel Povey</a>. I was one of the core contributors to the open-source <a href="https://github.com/k2-fsa/icefall">icefall</a> toolkit, deeply involved in <a href="https://arxiv.org/pdf/2310.11230.pdf">Zipformer</a> and <a href="https://arxiv.org/pdf/2309.08105.pdf">Libriheavy</a>, and I led projects on self-supervised speech representation learning (<a href="https://arxiv.org/pdf/2411.17100">continuous</a> and <a href="https://arxiv.org/pdf/2309.07377.pdf">discrete</a>).
								</p>
								<p>
									My research is supported by <a href="https://www.cie.org.cn/list_43/14514.html">CIE-Tencent Doctoral Research Incentive Project</a>. I am always open to collaboration and discussion. Please feel free to reach out.
								</p>
								<h3>Interests</h3>
								<ul>
									<li>
										<p>Text-to-Speech Synthesis and Evaluation</p>
									</li>
									<li>
										<p>Speech Representation Learning / Speech Tokenization (Continuous and Discrete)</p>
									</li>
									<li>
										<p>Multilingual Speech Recognition</p>
									</li>
								</ul>

								<h3>Education</h3>
								<ul>
									<li>
										<p>Ph.D., Computer Science and Technology, Shanghai Jiao Tong University, 2023.09-now</p>
									</li>
									<li>
										<p>B.E., Computer Science and Technology, Tianjin University, 2019.09-2023.07</p>
										<p>GPA: 3.91/4.0, Rank: 1/139. [<a href="https://yfyeung.github.io/CV/Transcript-en-undergraduate.pdf">Transcript</a>]</p>
									</li>
								</ul>

								<h3>Experiences</h3>
								<ul>
									<li>
										<p>Research Intern, Hunyuan Speech Team, Tencent TEG, 2025.08.20-now</p>
										<p>Investigate speech understanding for speaking style modeling and style-controllable text-to-speech.</p>
										<p>Co-advised by Dr. <a href="https://scholar.google.com/citations?user=ZnwgSXIAAAAJ">Long Zhou</a> and Dr. <a href="https://scholar.google.com/citations?user=tob-U1oAAAAJ">Xu Tan</a>.</p> 
									</li>
								</ul>
								<ul>
									<li>
										<p>Research Intern, <a href="https://www.microsoft.com/en-us/research/project/vall-e-x/people">VALL-E Team</a> & CoreAI Speech, Microsoft, 2024.03.05-2025.08.10</p>
										<p>Investigate advanced language modeling for text-to-speech synthesis and streaming text-to-speech synthesis.</p>
										<p>Co-advised by Dr. <a href="https://www.microsoft.com/en-us/research/people/shujliu">Shujie Liu</a> and Dr. <a href="https://www.microsoft.com/en-us/research/people/jinyli">Jinyu Li</a>.</p>
									</li>
								</ul>
								<ul>
									<li>
										<p>Machine Learning Engineer Intern, Next-gen Kaldi Team, Xiaomi AI Lab, 2022.11.01-2023.08.28</p>
										<p>Investigate advanced and efficient open-source end-to-end automatic speech recognition.</p>
										<p>Develop the <a href="https://k2-fsa.org">Next-gen Kaldi</a>, including <a href="https://github.com/k2-fsa/icefall">Icefall</a>, <a href="https://github.com/lhotse-speech/lhotse">Lhotse</a>, <a href="https://github.com/k2-fsa/k2">k2</a>.</p>
										<p>Advised by Dr. <a href="http://danielpovey.com/">Daniel Povey</a>.</p>
									</li>
								</ul>

								<h3>News</h3>
								<div style="max-height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 8px; background-color: #fefefe; box-shadow: 0 2px 5px rgba(0,0,0,0.1); margin-bottom: 20px;">
								    <ul style="list-style: none; margin: 0; padding: 0;">
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2026.01] 1 paper is accepted by ICASSP 2026.</p>
								        </li>
										<li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2026.01] 1 paper is accepted by IEEE JSTSP (IF=13.6).</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.08] I join the Hunyuan speech team in Tencent.</p>
								        </li>
										<li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.08] 1 paper is accepted by IEEE SPL.</p>
								        </li>
										<li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.07] I am honored to be funded by the <a href="https://www.cie.org.cn/list_43/14514.html">CIE-Tencent Doctoral Research Incentive Project</a>.</p>
								        </li>
										<li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.07] 3 papers are accepted by ACMMM 2025.</p>
								        </li>
										<li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.05] 2 papers are accepted by Interspeech 2025.</p>
								        </li>
										<li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.05] 3 papers are accepted by ACL 2025 (2 Main, 1 Findings).</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2025.03] 1 paper is accepted by ICME 2025.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2024.12] 1 paper is accepted by ICASSP 2025.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2024.12] 1 paper is accepted by AAAI 2025.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2024.06] 3 papers are accepted by Interspeech 2024.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2024.03] I join the speech team in Microsoft</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2024.01] <a href="https://arxiv.org/pdf/2310.11230.pdf">Zipformer</a> is accepted for <span style="color:red; font-weight:bold;">oral</span> presentation by ICLR 2024. Congratulations!</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2023.12] 3 papers are accepted by ICASSP 2024.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2023.09] I start to pursue my Ph.D. at Shanghai Jiao Tong University.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2023.06] I earn my Bachelor's degree in engineering with an excellent student title.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2023.05] 2 papers are accepted by Interspeech 2023.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2022.11] I join the Next-gen Kaldi team in Xiaomi.</p>
								        </li>
								        <li style="margin-bottom: 8px;">
								            <p style="margin: 0;">[2022.06] I join <a href="https://x-lance.github.io/">X-LANCE</a> lab in Shanghai Jiao Tong University.</p>
								        </li>
								    </ul>
								</div>

								<header class="major">
								<h2>Research</h2>
								</header>
								
								<h3>Selected Publications</h3>
								<p>Check out full publications on <a href="https://scholar.google.com/citations?user=slhAlQ0AAAAJ">Google Scholar</a>.</p>
								<h4>Zero-Shot Text-to-Speech Synthesis and Evaluation</h4>
								<ul>
									<li>
										<p><a href="https://arxiv.org/pdf/2504.10352">Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis</a></p>
										<p><b>Yifan Yang</b>, Shujie Liu, Jinyu Li, Yuxuan Hu, Haibin Wu, Hui Wang, Jianwei Yu, Lingwei Meng, Haiyang Sun, Yanqing Liu, Yan Lu, Kai Yu, Xie Chen</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. ACMMM 2025</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2412.16102">Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis</a></p>
										<p><b>Yifan Yang</b>, Shujie Liu, Jinyu Li, Hui Wang, Lingwei Meng, Haiyang Sun, Yuzhe Liang, Ziyang Ma, Yuxuan Hu, Rui Zhao, Jianwei Yu, Yan Lu, Xie Chen</p>
										<p>Preprint in arXiv, 2024</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2509.19928">Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration</a></p>
										<p><b>Yifan Yang</b>, Bing Han, Hui Wang, Long Zhou, Wei Wang, Mingyu Cui, Xu Tan, Xie Chen</p>
										<p>Proc. ICASSP 2026</p>
									</li>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2510.06927">Towards Responsible Evaluation for Text-to-Speech</a></p>
										<p><b>Yifan Yang</b>, Hui Wang, Bing Han, Shujie Liu, Jinyu Li, Yong Qin, Xie Chen</p>
										<p>Preprint in arXiv, 2025</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2506.12570">StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling</a></p>
										<p>Hui Wang, <b>Yifan Yang</b>, Shujie Liu, Jinyu Li, Lingwei Meng, Yanqing Liu, Jiaming Zhou, Haoqin Sun, Yan Lu, Yong Qin</p>
										<p>IEEE Signal Processing Letters</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2510.14664">SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality Evaluation</a></p>
										<p>Hui Wang, Jinghua Zhao, <b>Yifan Yang</b>, Shujie Liu, Junyang Chen, Yanzhe Zhang, Shiwan Zhao, Jinyu Li, Jiaming Zhou, Haoqin Sun, Yan Lu, Yong Qin</p>
										<p>Preprint in arXiv, 2025</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2502.11128">FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching</a></p>
										<p>Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, <b>Yifan Yang</b>, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. ACMMM 2025</p>
									</li>
								</ul>
								<h4>Speech Representation Learning</h4>
								<ul>
									<li>
										<p><a href="https://arxiv.org/pdf/2309.07377.pdf">Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS</a></p>
										<p><b>Yifan Yang</b>, Feiyu Shen, Chenpeng Du, Ziyang Ma, Kai Yu, Daniel Povey, Xie Chen</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. ICASSP 2024</p>
										<p>[<a href="https://github.com/k2-fsa/icefall/pull/1248">Code</a>] [<a href="https://yfyeung.github.io/Slides/Slides_DiscreteSpeech.pdf">Slides</a>]</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2411.17100">k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning</a></p>
										<p><b>Yifan Yang</b>, Jianheng Zhuo, Zengrui Jin, Ziyang Ma, Xiaoyu Yang, Zengwei Yao, Liyong Guo, Wei Kang, Fangjun Kuang, Long Lin, Daniel Povey, Xie Chen</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. ICME 2025</p>
										<p>[<a href="https://github.com/k2-fsa/icefall/pull/1745">Code</a>] [<a href="https://yfyeung.github.io/Slides/Slides_k2SSL.pdf">Slides</a>]</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2601.03065">Towards Fine-Grained and Multi-Granular Contrastive Language-Speech Pre-training</a></p>
										<p><b>Yifan Yang</b>, Bing Han, Hui Wang, Wei Wang, Ziyang Ma, Long Zhou, Zengrui Jin, Guanrou Yang, Tianrui Wang, Xu Tan, Xie Chen</p>
										<p>Preprint in arXiv, 2026</p>
										<p>[<a href="https://github.com/yfyeung/CLSP">Code</a>]</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2510.25955">SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations</a></p>
										<p>Xiaoyu Yang, <b>Yifan Yang</b>, Zengrui Jin, Ziyun Cui, Wen Wu, Baoxiang Li, Chao Zhang, Phil Woodland</p>
										<p>Preprint in arXiv, 2025</p>
										<p>[<a href="https://huggingface.co/collections/marcoyang/spear-encoders">Model</a>]</p>
									</li>
								</ul>
								<h4>Speech Recognition</h4>
								<ul>
								    <li>
										<p><a href="https://arxiv.org/pdf/2406.11546">GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement</a></p>
										<b>Yifan Yang</b>, Zheshu Song, Jianheng Zhuo, Mingyu Cui, Jinpeng Li, Bo Yang, Yexing Du, Ziyang Ma, Xunying Liu, Ziyuan Wang, Ke Li, Shuai Fan, Kai Yu, Wei-Qiang Zhang, Guoguo Chen, Xie Chen</p>
										<p>Proc. ACL 2025 Main</p>
										<p>[<a href="https://huggingface.co/datasets/speechcolab/gigaspeech2">Dataset</a>] [<a href="https://github.com/SpeechColab/GigaSpeech2">Code</a>] [<a href="https://yfyeung.github.io/Slides/Slides_GigaSpeech_2.pdf">Slides</a>]</p>
									    <p>GigaSpeech 2 powers <a href="https://github.com/DataoceanAI/Dolphin">Dolphin</a>, which represents the state-of-the-art multilingual and multitask ASR model for Eastern languages.</p>
										<p>GigaSpeech 2 powers <a href="https://huggingface.co/collections/typhoon-ai/typhoon-asr">Typhoon ASR series</a>(Typhoon ASR Real-Time, Typhoon Whisper, etc), which represents the state-of-the-art Thai ASR models.</p>
									</li>
									<li>
										<p><a href="https://www.isca-speech.org/archive/pdfs/Interspeech_2023/yang23l_Interspeech.pdf">Blank-regularized CTC for Frame Skipping in Neural Transducer</a></p>
										<p><b>Yifan Yang</b>, Xiaoyu Yang, Liyong Guo, Zengwei Yao, Wei Kang, Fangjun Kuang, Long Lin, Xie Chen, Daniel Povey</p>
										<p>Proc. Interspeech 2023</p>
										<p>[<a href="https://github.com/k2-fsa/icefall/pull/933">Code</a>]</p>
									</li>
									<li>
										<p><a href="https://www.isca-archive.org/Interspeech_2024/jin24_Interspeech.pdf">LibriheavyMix: A 20,000-Hour Dataset for Single-Channel Reverberant Multi-Talker Speech Separation, ASR and Speaker Diarization</a></p>
										<p>Zengrui Jin*, <b>Yifan Yang*</b>, Mohan Shi*, Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Liyong Guo, Lingwei Meng, Long Lin, Yong Xu, Shi-Xiong Zhang, Daniel Povey</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. Interspeech 2024</p>
										<p>[<a href="https://huggingface.co/zrjin?search_datasets=libriheavymix">Dataset</a>]</p>
									</li>
									<li>
										<p><a href="https://www.isca-archive.org/interspeech_2025/zhuo25_interspeech.pdf">VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining</a></p>
										<p>Jianheng Zhuo, <b>Yifan Yang</b>, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen</p>
										<p>Proc. Interspeech 2025</p>
										<p>[<a href="https://github.com/zzasdf/VietASR">Code</a>]</p>
									</li>
									<li>
										<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/34666">Speech Recognition Meets Large Language Model: Benchmarking, Models, and Exploration</a></p>
										<p>Ziyang Ma, Guanrou Yang, <b>Yifan Yang</b>, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. AAAI 2025</p>
										<p>[<a href="https://github.com/X-LANCE/SLAM-LLM/blob/main/examples/asr_librispeech">Code</a>]</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2309.08105.pdf">Libriheavy: A 50,000 hours ASR Corpus with Punctuation Casing and Context</a></p>
										<p>Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, <b>Yifan Yang</b>, Liyong Guo, Long Lin, Daniel Povey</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. ICASSP 2024</p>
										<p>[<a href="https://huggingface.co/datasets/pkufool/libriheavy">Dataset</a>] [<a href="https://github.com/k2-fsa/libriheavy">Code</a>]</p>
									</li>
									<li>
										<p><a href="https://arxiv.org/pdf/2310.11230.pdf">Zipformer: A Faster and Better Encoder for Automatic Speech Recognition</a></p>
										<p>Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, <b>Yifan Yang</b>, Zengrui Jin, Long Lin, Daniel Povey</p>
										<p><span style="color:red; font-weight:bold;">Oral</span> in Proc. ICLR 2024</p>
										<p>[<a href="https://github.com/k2-fsa/icefall/tree/master/egs/librispeech/ASR/zipformer">Code</a>]</p>
									</li>
								</ul>
								
								<h3>Open-Source Projects</h3>
								<ul>
									<li>
										<p><a href="https://github.com/k2-fsa/icefall">Icefall: The recipes of the Next-gen Kaldi</a></p>
									</li>
									<li>
										<p><a href="https://github.com/lhotse-speech/lhotse">Lhotse: Tools for handling speech data in machine learning projects</a></p>
									</li>
								</ul>

								<h3>Awards</h3>
								<ul>
									<li>
										<p><a href="https://www.cie.org.cn/list_43/15228.html">Hunyuan Fellowship</a>, China Institute of Electronics & Tencent, 2025</p>
									</li>
									<li>
										<p><a href="http://foundation.byd.com/foundation/Edu-Charity.html">BYD Scholarship</a>, BYD, 2025</p>
									</li>
									<li>
										<p>Chu Xin Scholarship, Tianjin University, 2022</p>
									</li>
									<li>
										<p><a href="http://www.bsef.baosteel.com/#/aboutus">Baosteel Scholarship</a>, Baosteel Education Foundation, 2021</p>
									</li>
									<li>
										<p>“Bingchang Zhuang” Scholarship, Tianjin University, 2020</p>
									</li>
								</ul>
								
								<h3>Academic Service</h3>
								<h4>Conference Reviewer</h4>
								<ul>
									<li>
										<p>International Conference on Machine Learning (ICML 2026)</p>
									</li>
									<li>
										<p>International Conference on Learning Representations (ICLR 2026, <a href="https://iclr.cc/Conferences/2025/Reviewers" target="_blank" style="color:red; font-weight:bold;">Notable Reviewer</a> at 2025)</p>
									</li>
									<li>
										<p>AAAI Conference on Artificial Intelligence (AAAI 2026)</p>
									</li>
									<li>
										<p>ACL Rolling Review (ACL ARR 2026 January, 2025 October, 2025 May, 2025 February, 2024 December, 2024 October, 2024 June, 2023 October)</p>
									</li>
									<li>
										<p>Conference on Neural Information Processing Systems (NeurIPS 2025)</p>
									</li>
									<li>
										<p>ACM International Conference on Multimedia (ACM MM 2025)</p>
									</li>
									<li>
										<p>International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026, 2025, 2024)</p>
									</li>
									<li>
										<p>The Annual Conference of the International Speech Communication Association (Interspeech 2026)</p>
									</li>
									<li>
										<p>IEEE International Conference on Multimedia & Expo (ICME 2026, 2025)</p>
									</li>
									<li>
										<p>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2025)</p>
									</li>
									<li>
										<p>International Conference on Computational Linguistics (COLING 2025, LREC-COLING 2024)</p>
									</li>
									<li>
										<p>IEEE Spoken Language Technology Workshop (SLT 2024)</p>
									</li>
									<li>
										<p>Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)</p>
									</li>
								</ul>
								<h4>Journal Reviewer</h4>
								<ul>
									<li>
										<p>IEEE Transactions on Audio, Speech and Language Processing (IEEE TASLP)</p>
									</li>
									<li>
										<p>IEEE Open Journal of Signal Processing (IEEE OJSP)</p>
									</li>
								</ul>

								<h3>Activities</h3>
								<ul>
									<li>
										<p>[Invited Talk]  Open-source Sharing of F5-TTS and GigaSpeech 2, <a href="https://modelscope.cn/DevCon2025">ModelScope DevCon 2025</a>, 2025.06</p>
									</li>
									<li>
										<p>[Invited Talk]  GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement, Nanyang Technological University (NTU), 2024.06</p>
									</li>
									<li>
										<p><a href="https://github.com/CS-BAOYAN">CS-BAOYAN</a> Owner, the largest nonprofit CS postgraduate recommendation exchange platform in China,  2022.09-2023.09</p>
									</li>
								</ul>

								<h3>Teaching Assistance</h3>
								<ul>
									<li>
										<p>SJTU CS1501 Programming</p>
									</li>
								</ul>

								<ul>
									<a href='https://clustrmaps.com/site/1but5'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=xmZZKuR9JgwM-nnqvhx7hQETXCchJo7zQhRldlQGf6s'/></a>
								</ul>
							</section>
						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<nav id="menu">
									<header class="major">
										<h2>About me</h2>
									</header>
									<ul>
										<li><a href="https://scholar.google.com/citations?user=slhAlQ0AAAAJ">Scholar</a></li>
										<li><a href="https://github.com/yfyeung/">GitHub</a></li>
										<li><a href="https://huggingface.co/yfyeung">Huggingface</a></li>
										<li><a href="https://www.linkedin.com/in/yifan-yang-290ba624b/">LinkedIn</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="mailto:yifanyeung@sjtu.edu.cn">yifanyeung@sjtu.edu.cn</a></li>
										<li class="icon brands fa-weixin"><a href="images/wechat.JPG">WeChat</a></li>
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>
			</div>
		
		
		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>



